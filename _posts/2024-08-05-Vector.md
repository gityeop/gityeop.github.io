---
title: Vector
date: 2024-08-05
categories: math-for-ml
---

## 벡터가 뭔가요?

벡터는 숫자를 원소로 가지는 리스트(list) 또는 배열(array)

```python
x = [1, 7, 2]
x = np.array([1, 7, 2])
```

<div class="answer">
  <button class="toggle-answer">답안 보기/숨기기</button>
  <div class="answer-content" style="display: none;">
    <strong>답안:</strong>
    <p>$$
\mathbf{x}^T = [x_1, x_2, \ldots, x_d]
$$</p>
  </div>
</div>

d : 벡터의 차원

- 벡터는 공간에서는 <span class="blindfold" data-hint="">점</span>을 나타냄
- 벡터는 원점으로부터 <span class="blindfold" data-hint="">상대적 위치</span>를 표현
- 숫자를 곱해주면 길이만 변함(scalar)
- 벡터의 덧셈과 뺄셈: 벡터의 각 구성 성분끼리 더하거나 빼는 연산. 같은 차원의 벡터끼리만 연산이 가능
- 성분곱: 각 성분끼리 곱하는 연산

### 벡터의 덧셈

두 벡터의 덧셈은 다른 벡터로부터 상대적 위치 이동을 표현함

### 벡터의 뺄셈

방향을 뒤집은 덧셈

### 벡터의 노름(norm) 구해보기

- L1 노름: 각 성분의 <span class="blindfold" data-hint="">절대값</span>을 모두 더한 값으로, 좌표축을 따라 이동한 거리로 이해할 수 있음
- L2 노름: 피타고라스 정리에 의해 계산되는 거리로, 유클리드 거리라고도 불림

```python
def l1_nore(x):
    x_norm = np.abs(x)
    x_norm = np.sum(x_norm)
    return x_norm

def l2_norm(x):
    x_norm = x*x
    x_norm = np.sum(x_norm)
    x_norm = np.sqrt(x_norm)
    return x_norm
```

노름의 종류에 따라 기하학적 성질이 달라짐

- L1 norm : 단위 원이 마름모 형태

  - <span class="blindfold" data-hint="">이상치</span>에 덜 민감하여 <span class="blindfold" data-hint="">로버스트 회귀</span>와 같은 상황에 사용됨
  - 라소 회귀(LASSO regression) : 모델의 가중치 일부를 0으로 만들어 특성 선택(feature selection)을 수행함
    - <span class="blindfold" data-hint="">Dropout</span>과의 차이점 : 라소 회귀는 주로 선형 모델에 사용, 일부 특성을 0으로 만듬, 반면 Dropout은 신경망에 사용되고, 무작위로 뉴런을 비활성화

- L2 norm : 단위 원이 원형
  - 모든 데이터 포인트의 평균에 민감, 평균 제곱 오차(MSE)를 최소화하는 방법으로 사용됨
  - 릿지 회귀(Ridge regression) : 모델의 가중치 크기를 줄여 과적합을 방지

### 두 벡터 사이의 거리

벡터의 뺄셈을 통해 두 벡터 사이의 거리를 계산 L1 노름과 L2 노름을 이용하여 각각 다른 거리를 계산할 수 있음

### 두 벡터 사이의 각도

내적(inner product)과 L2 노름을 이용하여 두 벡터 사이의 각도를 계산함. 내적은 두 벡터의 유사도를 측정하는 데 사용됨

```python
def angle(x, y):
    v = np.inner(x, y) / (l2_norm(x) * l2_norm(y))
    theta = np.arccos(y)
    return theta
```

### 내적의 해석

- 정사형 벡터: 내적은 한 벡터가 다른 벡터 상에 투영(projection)된 길이와 관련이 있음
- 유사도 측정: 내적을 통해 두 벡터가 얼마나 비슷한 방향을 가지는지 측정할 수 있음
