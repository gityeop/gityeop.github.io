---
title: "모델 경량화 기법: Pruning(가지치기)"
date: 2024-12-24
categories: machine-learning
---


## 1. 프루닝 개념

**프루닝**은 신경망 모델에서 **중요도가 낮은 뉴런이나 연결(시냅스)을 제거**하여 모델의 크기를 줄이고 계산 비용을 감소시키는 **모델 경량화 기법**이다. 프루닝은 **노드(뉴런)**, **간선(시냅스)**, **레이어** 등 다양한 단위로 수행할 수 있다.

### 현실 세계에서의 프루닝 단계
1. **완전한 모델**: 모든 파라미터 값을 가지고 있는 초기 모델
2. **제거 대상 판별**: 중요도가 낮은 파라미터들을 식별 (알고리즘 사용)
3. **프루닝**: 식별된 파라미터들을 제거 (0으로 대체)
4. **튜닝**: 제거로 인한 성능 저하를 보완하기 위해 모델 재학습
5. **평가 및 배포**: 원하는 조건을 만족하는 경우 모델 배포

### 프루닝의 목표
- **메모리 사용량 감소**: 파라미터 저장 공간을 줄임으로써 메모리 사용량 감소
- **연산 속도 가속화**: 파라미터 수 감소로 계산량이 줄어들어 연산 속도 향상
- **에너지 절약**: 메모리 사용량 및 연산량 감소로 에너지 소비 감소

### 프루닝의 효과
- 계산해야 할 **파라미터 수 감소**
- **연산 속도 향상**
- **메모리 사용량 감소**
- **에너지 절약**
- **모델 경량화**와 동시에 성능 유지, 경우에 따라 성능 향상 가능
- **드롭아웃**과 같은 **regularization** 효과를 통해 과적합 방지 및 성능 개선 가능

> 프루닝 비율이 지나치게 높아지면 성능 저하가 발생한다 (적절한 비율 설정이 중요하다)

## 2. 프루닝 방법론 분류

프루닝 방법론은 크게 **4가지 관점**으로 분류할 수 있다.

### (1) Structure (구조)
프루닝을 어떤 단위로 진행할 것인지 결정한다.

#### Unstructured Pruning
- **개별 파라미터** 단위로 0으로 변경
- 모델 구조 변경 없음
- 구현이 쉽다
- 가속 및 메모리 절약 효과가 즉각적이지 않다

#### Structured Pruning
- **채널**, **레이어** 등 특정 구조 단위를 통째로 제거
- 모델 구조 변경이 발생
- **즉각적인 가속** 및 **메모리 효율** 증대
- 구현이 어렵다

### (2) Scoring (점수 매기기)
프루닝할 파라미터 선정 기준을 결정한다.

#### 중요도 계산 방법
- **절대값**: 파라미터의 절대값을 기준으로 중요도 측정. 절대값이 작을수록 중요도 낮음
- **레이어별 Lp-norm**: 레이어별 Lp-norm 값을 중요도로 사용. 값이 작을수록 중요도 낮음

#### 중요도 반영 방식
- **Global Pruning**: 전체 모델에서 중요도가 낮은 파라미터들을 일괄적으로 제거 (절대적 비교)
- **Local Pruning**: 레이어별로 중요도를 계산하여 각 레이어 내에서 제거 (상대적 비교)

### (3) Scheduling (일정)
프루닝 후 파인튜닝 시점 및 횟수를 결정한다.

#### One-shot Pruning
- 프루닝을 **한 번에 진행**하고 파인튜닝도 한 번만 수행
- **시간 절약** 가능
- 성능 불안정 가능성

#### Recursive Pruning
- 프루닝을 **여러 번 나누어** 진행하고 매 단계마다 파인튜닝 수행
- 시간 소요
- **안정적인 성능** 확보 가능
- **높은 프루닝 비율**에도 성능 저하 감소

### (4) Initialization (초기화)
파인튜닝 전 파라미터 초기화 방식을 결정한다.

#### Weight Preserving
- **기존 학습된 파라미터 값을 유지**한 채 파인튜닝 수행
- **빠른 수렴**
- 성능 저하 가능성 (local minimum 문제)

#### Weight Reinitialization
- 프루닝된 모델을 **랜덤 파라미터**로 초기화 후 재학습
- 재학습 시간 증가
- **높은 성능** 및 **안정성** 확보 가능

## 3. 복권 가설 (Lottery Ticket Hypothesis)

딥러닝 모델에서 **프루닝의 가능성**을 최초로 실험적으로 증명한 논문
초기 모델에서 일부 파라미터 (**winning ticket**) 만으로도 전체 모델과 유사한 성능 달성 가능

### 핵심 과정
1. **초기 모델 학습**
2. **프루닝**을 통해 작은 모델 생성
3. 작은 모델의 파라미터를 **초기 값으로 초기화**
4. 작은 모델 **재학습**
5. 작은 모델과 전체 모델의 **성능 비교** (거의 유사한 성능 확인)

## 4. Iterative Magnitude Pruning (IMP)

위에서 설명된 4가지 텍사노미를 적용한 프루닝 기법

### 특징
- **Unstructured Pruning** 방식 사용
- 파라미터의 **절대값**을 기준으로 중요도 측정 (magnitude pruning)
- **Global Pruning** 방식 사용
- **Recursive Pruning** 방식 사용
- **Weight Reinitialization** 방식 사용

### IMP 과정
1. **초기 모델 학습**
2. **절대값** 기반으로 중요도 낮은 파라미터 제거 (global pruning)
3. **랜덤 파라미터**로 초기화 후 파인튜닝
4. 2~3단계 반복