# 확률론

- 딥러닝에서 확률론이 필요한 이유
  - 딥러닝은 확률론 기반의 기계학습 이론에 바탕을 두고 있음
  - 기계학습에서 사용되는 손실함수(loss function)들의 작동 원리는 데이터 공간을 통계적으로 해석해서 유도하게 됨
  - 회기 분석에서 손실함수로 나용되는 L2-노름은 예측오차의 분산을 가장 최소화하는 방향으로 학습하도록 유도함
  - 분류 문제에서 사용되는 교차엔트로피(cross-entropy)는 모델 예측의 불확실성을 최소화하는 방향으로 학습하도록 유도함
  - 분산 및 불확실성을 최소화하기 위해서는 측정하는 방법을 알아야 함 -함

## 확률분포틑 데이터의 초상화

- 데이터 공간을 x\*y라 표기하고 D는 데이터공간에서 데이터를 추출하는 분포
- 데이터는 확률변수로(x, y) ~ D라 표기

## 이산확률변수 vs 연속확률변수

- 확률 변수는 확률 분포 D에 따라 이산형(discrete)과 연속형(continuous)확률변수로 구분하게 됨(데이터 공간이 정수냐 실수냐 X)
- 이산형 확률변수는 확률 변수가 가질 수 있는 경우의 수를 모두 고려하여 확률을 더해서 모델링한다
- 연속형 확률변수는 데이터 공간에 정의된 확률변수의 밀도(density) 위에서의 적분을 통해 모델링한다
