## 확률론

- 기계학습에서 사용되는 손실함수(loss function)들의 작동원리는 데이터 공간을 통계적으로 해석해서 유도
  - e.g1) L2 norm : 예측 오차의 **분산**을 최소화하는 방향으로 학습
  - e.g2) 교차 엔트로피 : 모델 예측의 **불확실성**을 최소화하는 방향으로 학습
- 분산, 불확실성을 최소화하기 위해서는 측정하는 방법을 알아야 함

확률 분포 D : 데이터 공간 X*Y 상의 데이터를 표현하는 초상화
데이터, 확률변수, 데이터공간 상에서 관측가능한 데이터, X*Y으 원소로써 표시
확률 변수는 함수로 해석 : 임의로 데이터 공간에서 관측을 하게 되는 함수로

점 : 데이터 공간 상에서 관측한 데이터
데이터를 추출할 때 확률 변수를 사용
확률 변수로 추출한 데이터의 분포를 D

확률 변수는 확률분포 D에 따라 이산형(discrete) 연속형(continuous)/ 데이터 공간으로 결정되는 것이 아님

이산형 확률변수는 확률변수를 가질 수 있는 모든 경우의 수를 고려하여 확률을 더해서 모델링
=> 이산형 확률변수는 가능한 모든 값들에 대한 확률을 모두 더해서 전체 확률분포를 만듭니다.
확률 질량 함수 : 특정한 값에 대한 확률의 **크기**, 이산적이므로 각각의 값에 대한 특정한 **덩어리**의 확률, 확률의 크기 => 질량

연속형 확률변수는 데이터 공간에 정의된 확률변수의 밀도(density) 위에서의 적분을 통해 모델링
=> 연속형 확률변수는 특정 구간에서 값이 나올 확률을 계산하기 위해, 확률 밀도 함수를 사용하여 그 구간을 적분합니다.
확률 밀도 함수는 확률이 아니라, 누적 분포 함수가 얼마나 빨리 변하는지를 나타냅니다. 따라서, 특정 값에서의 밀도는 그 값 자체의 확률을 의미하지 않습니다.

확률 밀도 함수 (Probability Density Function, PDF)

확률 밀도 함수 f(x) 는 연속형 확률변수가 특정 값 근처에서 얼마나 자주 발생하는지를 나타내는 함수입니다. 그러나 PDF 자체는 특정 값에서의 확률을 직접적으로 나타내지는 않습니다.

누적 분포 함수 (Cumulative Distribution Function, CDF)

누적 분포 함수 F(x) 는 특정 값 이하의 확률을 나타내는 함수입니다. 즉, X 가 x 이하일 확률을 나타냅니다:
$$ F(x) = P(X \leq x) $$

PDF와 CDF의 관계

누적 분포 함수 F(x) 는 확률 밀도 함수 f(x) 를 적분하여 얻을 수 있습니다. 구체적으로는 다음과 같은 관계가 있습니다:
$$ F(x) = \int\_{-\infty}^{x} f(t) \, dt $$

---

어떤 경우엔 이산형, 어떤 경우엔 연속형 확률 분포를 가지는 확률 변수도 있음
-> 혼합분포(mixed distribution)
e.g. 통근 시간
지하철을 타는 사람들 중 일부는 특정 시간에 맞춰서 탑승. 지하철 출발 시간 -> 이산형 확률 변수
다른 교통 수단, 자가용 -> 도착 시간이 연속적으로 변할 수 있음
혼합 분포는 이산형 확률 분포와 연속형 확률 분포의 결합으로 표현됩니다. 일반적으로 혼합 분포는 다음과 같은 형태를 가집니다:
$$ P(X = x) = p \cdot P*{d}(X = x) + (1 - p) \cdot f*{c}(x) $$
여기서:

    -	 $ P_{d}(X = x) $는 이산형 확률 분포 부분
    -	 $ f_{c}(x)  $는 연속형 확률 분포 부분 (PDF)
    -	 $ p $ 는 이산형 분포의 가중치 (0 ≤ p ≤ 1)

- $$ f\_{c}(x) = \frac{1}{\sqrt{2\pi\sigma^2}} e^{-\frac{(x-\mu)^2}{2\sigma^2}} $$

---

공동확률분포 (Joint Probability Distribution)

공동확률분포는 두 개 이상의 확률변수가 동시에 발생하는 확률을 나타냅니다. 예를 들어, 두 확률변수 X 와 Y 가 있을 때, 이들의 공동확률분포 P(X, Y) 는 X 와 Y 가 동시에 특정 값을 가질 확률을 나타냅니다.

모델링 방법에 따라 원래 데이터가 이산이더라도 결합 분포를 연속으로 할 수 있음.(vice versa)

주변확률분포

주변확률분포(Marginal Probability Distribution)는 다변량 확률분포에서 특정 변수에 대한 확률분포를 의미합니다. 이를 통해 관심 있는 변수에 대한 정보를 얻을 수 있습니다.

조건부확률분포

조건부확률분포(Conditional Probability Distribution)는 어떤 사건이 주어졌을 때 다른 사건이 발생할 확률을 나타내는 분포입니다. 이는 특정 조건이 성립하는 상황에서의 확률분포를 계산하는 데 사용됩니다.

조건부확률 P(y|x)는 입력변수 x에 대해 정답이 y일 확률을 의미함
로지스틱 회귀에서 사용했던 선형모데로가 소프트맥스 함수의 결합은 데이터에서 추출된 패턴을 기반으로 확률을 해석하는데 사용됨
분류 문제에서 softmax(w\*피x+b)는 데이터 x로부터 추출된 특징패턴 피x와 가중치 행렬 W를 통해 조건부확률 P(y|x)를 계산합니다

> 입력 데이터 x 에 대해 각 클래스 y 에 속할 확률 P(y|x)

$$ P(y = 1 | x) = \sigma(w^T x + b) $$

입력 데이터에 대해 출력의 확률분포를 학습하고 예측한다는 것은:

    •	학습: 주어진 입력 데이터  x 에 대해, 가능한 출력  y 가 각각 얼마나 나올지를 학습합니다. 이는 각 출력  y 에 대한 확률을 학습하는 과정입니다.
    •	예측: 학습된 모델을 사용하여 새로운 입력 데이터  x 가 주어졌을 때, 가능한 출력  y 들이 각각 얼마나 나올지를 예측합니다. 즉, 각 출력  y 에 대한 조건부 확률  P(y|x) 를 예측합니다.

합성함수의 관점에서 머신러닝은 입력 데이터를 여러 단계의 함수 변환을 통해 출력으로 매핑하는 과정으로 볼 수 있습니다.

확률론의 관점에서 머신러닝은 데이터와 모델의 불확실성을 다루고, 주어진 입력 데이터에 대한 출력의 확률분포를 학습하는 과정으로 볼 수 있습니다.

    •	소프트맥스 회귀: 조건부확률을 사용하여 각 클래스에 속할 확률을 계산하고, 그 중 가장 높은 확률을 가진 클래스를 최종 예측으로 선택합니다. 예를 들어, 이미지 분류에서 고양이일 확률이 0.7, 개일 확률이 0.2, 새일 확률이 0.1이라면, 고양이 클래스를 최종 예측으로 선택합니다.
    •	선형 회귀: 조건부기대값을 사용하여 입력 값  x 에 대해 예상되는 출력 값  y 의 평균을 예측합니다. 예를 들어, 주택 가격 예측에서 입력 값이 특정 지역의 특징이라면, 해당 지역의 평균 주택 가격을 예측합니다.

---

연속확률변수의 경우 적분으로 표현, 그럼 왜 조건부기대값을 사용하느냐? 회귀문제의 경우 사용하는 사용하는 손실함수가 L2노름의 기대값, 조건부기대값은 이 L2 노름을 최소화하는 함수와 일치

상세 설명

    1.	각각의 요소 설명:
    •	 y : 실제 집 가격.
    •	 \hat{y} : 모델이 예측한 집 가격.
    •	 p(y|x) : 주어진 집 크기  x 에 대해  y 가 특정 가격일 확률 밀도.
    •	 (y - \hat{y})^2 : 예측 가격과 실제 가격 사이의 제곱 오차.
    •	적분  \int :  y 의 모든 가능한 값에 대해 계산을 수행함.
    2.	적분의 과정:
    •	단계 1: 각 가능한 집 가격  y 에 대해 제곱 오차  (y - \hat{y})^2 를 계산합니다.
    •	단계 2: 이 제곱 오차에  y 가 나올 확률  p(y|x) 를 곱합니다. 이는 각 가격에 대해 얼마나 자주 그 가격이 나올지를 고려하는 것입니다.
    •	단계 3: 모든 가능한 집 가격  y 에 대해 이 과정을 반복하고, 그 결과를 적분을 통해 모두 더하여 평균적인 손실을 구합니다.

예시를 통해 이해하기

    •	집 크기  x  = 100 평방미터인 경우:
    •	가격 분포  p(y|x) :
    •	20만 달러일 확률: 0.1
    •	25만 달러일 확률: 0.5
    •	30만 달러일 확률: 0.4
    •	예측 가격  \hat{y}  = 28만 달러

적분을 통해 계산된 손실:
L2 = \int (y - 280,000)^2 p(y|x) \, dy

이를 구체적으로 계산해보면:

    •	 y = 200,000 일 때, 제곱 오차는  (200,000 - 280,000)^2 = 6,400,000,000
    •	 y = 250,000 일 때, 제곱 오차는  (250,000 - 280,000)^2 = 900,000,000
    •	 y = 300,000 일 때, 제곱 오차는  (300,000 - 280,000)^2 = 400,000,000

각 제곱 오차에 해당 확률을 곱합니다:

    •	20만 달러일 확률:  6,400,000,000 \times 0.1 = 640,000,000
    •	25만 달러일 확률:  900,000,000 \times 0.5 = 450,000,000
    •	30만 달러일 확률:  400,000,000 \times 0.4 = 160,000,000

모든 값을 더하면:
640,000,000 + 450,000,000 + 160,000,000 = 1,250,000,000

---

기대값이 뭔가요?

- 확률분포가 주어지면 데이터를 분석하는 데 사용 가능한 여러 종류의 통계적 범함수를 계산할 수 있음
- 기대값을 통해 분산, 첨도, 공분산 등 여러 통계량을 계산할 수 있음

분산의 정의를 이산형 확률변수의 형태로 전개

    1.	기대값 $ E[X] $ 계산:

$$ E[X] = \sum*{i} x_i \cdot P(X = x_i) $$
여기서 $ x_i $는  $X $가 가질 수 있는 값이고,  $ P(X = x_i) $ 는 그 값이 나올 확률입니다.
	2.	분산의 정의:
 $$ \text{Var}(X) = E[(X - E[X])^2] $$
	3.	편차의 제곱:
각  $x_i$ 에 대해 편차 $ (x_i - E[X]) $의 제곱을 계산합니다.
	4.	제곱 편차의 기대값 계산:
이산형 확률변수의 기대값을 계산하는 방식을 사용하여 제곱 편차의 기대값을 구합니다:
 $$ \text{Var}(X) = \sum*{i} (x_i - E[X])^2 \cdot P(X = x_i) $$

최종 표현

이렇게 하면 분산을 이산형 확률변수의 형태로 표현할 수 있습니다:
$$ \text{Var}(X) = \sum\_{i} (x_i - E[X])^2 \cdot P(X = x_i) $$

---

딥러닝은 다층신경망을 사용하여 데이터로부터 특징패턴을 추출한다.
특징패턴을 학습하기 위해 어떤 손실함수를 사용할지는 기계학습 문제와 모델에 의해 결정된다

딥러닝과 손실함수의 예시

    •	회귀 문제: 주택 가격 예측에서 입력 데이터(예: 집의 크기, 위치 등)와 실제 집 가격 간의 차이를 최소화하기 위해 평균 제곱 오차(MSE)를 손실함수로 사용합니다.

$$ MSE = \frac{1}{n} \sum*{i=1}^{n} (y_i - \hat{y}\_i)^2 $$
	•	분류 문제: 이미지 분류에서 입력 이미지가 특정 클래스(예: 고양이, 개 등)에 속할 확률을 예측하기 위해 교차 엔트로피 손실을 사용합니다.
$$ \text{Cross-Entropy Loss} = -\sum*{i} y_i \log(\hat{y}\_i) $$

---

몬테카를로 샘플링

- 기계학습의 많은 문제들은 확률분포를 명시적으로 모를 때가 대부분
- 확률분포를 모를 때 데이터를 이용하여 기대값을 계산하려면 몬테카를로(Monte Carlo) 샘플링 방법을 사용해야 한다.
- 몬테카를로는 이산형이든 연속형이든 상관없이 성립한다

샘플링의 의미

    •	샘플링(Sampling): 전체 데이터나 확률분포에서 일부 값을 무작위로 선택하는 과정입니다. 이는 전체 분포의 특성을 이해하거나 계산을 단순화하는 데 사용됩니다.

몬테카를로 샘플링의 과정

몬테카를로 샘플링은 다음과 같은 과정을 통해 기대값을 추정합니다:

    1.	무작위 샘플링: 확률분포에서 무작위로 샘플을 추출합니다. 이때 이산형 확률분포든 연속형 확률분포든 상관없습니다.
    2.	기대값 계산: 추출한 샘플들을 사용하여 기대값을 계산합니다. 추출한 샘플들이 확률분포를 대표한다고 가정하고, 이를 통해 전체 분포의 특성을 추정합니다.
