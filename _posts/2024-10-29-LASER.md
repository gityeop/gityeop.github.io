---
title: LASER Language-Agnostic Sentence Representations
date: 2024-10-29
categories: machine-learning
tags: ["ai/nlp", "ai/embeddings", "ai/multilingual"]
excerpt: "언어 독립적 문장 표현"
---

## Language-Agnostic Sentence Representations

Facebook AI Research는 2017년에 발표한 "Language-Agnostic Sentence Representations" 연구를 통해, 다양한 언어에 걸쳐 통일된 문장 표현을 학습하는 방법을 제시하였다. 이 연구는 Neural Machine Translation(NMT)의 프레임워크를 사용하여 다중 언어의 문장을 공통된 벡터 공간에 표현하는 모델을 제안하였다. 이를 통해 다국어 간 의미의 일관성을 유지하며 문장을 비교할 수 있는 새로운 방식이 등장한 것이다. 이 글에서는 이 접근법의 동기, 작동 원리, 그리고 다른 방법론과의 비교 등을 구체적인 예시와 함께 이해하기 쉽게 설명하고자 한다.

### **다국어 문장 표현의 필요성과 정의**

다국어 문장 표현이란 같은 의미를 가진 서로 다른 언어의 문장들을 하나의 벡터 공간에서 가깝게 위치하도록 학습하는 것을 의미한다. 예를 들어 "A horse on a beach"라는 영어 문장과, 그에 해당하는 불어 문장 "Un cheval sur une plage"는 공통된 의미를 가진다. 이러한 문장들을 각각 벡터화하였을 때, 벡터 공간에서 서로 가깝게 위치한다면 동일한 의미를 잘 반영한 것이라 할 수 있다.

이러한 방식의 장점은 다국어 검색이나 번역과 같은 언어 처리 작업에서 매우 효율적이라는 점이다. 하나의 언어로 문장을 표현하더라도 다른 언어에서 그와 유사한 의미를 가진 문장을 손쉽게 찾을 수 있게 된다.

### **Neural Machine Translation과 Sentence Embeddings**

Facebook AI는 Neural Machine Translation(NMT) 기술을 사용하여 다국어 문장 표현을 학습하였다. NMT는 보통 시퀀스-투-시퀀스(sequence-to-sequence) 모델을 이용하여 하나의 언어에서 다른 언어로 문장을 번역하는 방식이다. 여기서 문장을 벡터로 표현하는 부분을 "인코더(encoder)"라고 하고, 이 벡터로부터 번역된 문장을 생성하는 부분을 "디코더(decoder)"라고 한다.

다국어 문장 표현을 학습하기 위해 Facebook은 "다중 인코더와 디코더(multiple encoders and decoders)" 방식을 사용하였다. 각 언어에 대해 인코더와 디코더를 별도로 두고, 각 언어가 소스 언어와 타겟 언어로 역할을 교차하면서 학습하였다. 연구에서 "우리는 각 소스 및 타겟 언어에 대해 인코더와 디코더를 사용한다"라고 제안하였다. 또한 "다중 입력 언어의 개념은 음성 및 이미지와 같은 다양한 모달리티로 확장될 수 있다"고 언급하여, 텍스트뿐 아니라 다른 형태의 데이터에도 적용할 수 있음을 밝혔다.

### **작동 원리: 다중 인코더와 디코더**

Facebook의 연구에서는 다중 인코더와 디코더 구조를 사용하여 다국어 문장 표현을 학습하였다. 각 언어마다 개별 인코더와 디코더를 두고, 여러 언어 간의 번역을 통해 문장 표현을 학습한다. 예를 들어, 영어와 프랑스어가 주어졌을 때, 영어 문장을 인코더를 통해 벡터로 변환하고, 프랑스어 디코더를 통해 이를 번역하는 방식이다. 이러한 과정에서 모델은 각 언어의 의미적 특성을 벡터 공간에 공통적으로 표현하도록 학습하게 된다.

특히 연구에서는 "고정 크기의 표현(fixed-size representation)을 사용하여 입력 및 출력 시퀀스의 길이에 상관없이 비교 및 검색을 용이하게 한다"고 강조하였다. 이는 긴 문장보다는 일상적인 짧은 문장에서 더욱 효과적일 수 있다【6†source】. 또한 학습이 완료된 후에는 디코더를 더 이상 사용하지 않으며, 이를 통해 의미적으로 유사한 문장끼리 동일한 벡터 공간에서 가깝게 위치하도록 한다. 이러한 접근은 기존의 어텐션 메커니즘을 사용하지 않고도 문장 표현을 학습할 수 있게 한다.

### **구체적인 예시: 다양한 언어 간 의미 비교**

구체적인 예로, 영어 문장 "I did not find out why"를 생각해보자. 이 문장을 벡터화하고, 프랑스어와 스페인어 문장들 중에서 가장 유사한 의미를 가진 문장을 찾는 작업을 수행하면, 프랑스어 문장 "Je ne comprends pas pourquoi"와 스페인어 문장 "No entiendo por qué"가 높은 유사도를 가지는 문장으로 반환된다. 이는 모델이 다국어 간 의미를 잘 유지하는 공통된 문장 표현을 학습했다는 것을 보여준다.

또 다른 예시로, "All kinds of obstacles must be eliminated"라는 문장을 입력했을 때, "All kinds of barriers have to be removed"라는 문장을 유사한 문장으로 찾아낸다. 이 과정에서 사용된 벡터는 영어뿐 아니라 프랑스어, 스페인어, 러시아어 등 다른 언어로 번역된 문장들과도 유사성을 평가할 수 있는 공통된 표현으로 벡터화된다.

### **다른 방법론과의 비교**

기존의 다국어 문장 표현 방식 중 하나는 병렬 코퍼스를 사용하여 각각의 언어에 대해 개별적으로 벡터를 학습하는 방식이었다. 그러나 이러한 방식은 각 언어에 대해 별도의 학습이 필요하며, 특히 저자원 언어의 경우 충분한 데이터가 없을 때 한계가 뚜렷했다. 반면 Facebook의 접근법은 공통된 벡터 공간을 학습함으로써 저자원 언어에 대해서도 효과적으로 의미를 비교하거나 번역할 수 있는 가능성을 제시하였다.

또한 기존의 단어 단위 임베딩(word embeddings)과 비교했을 때, Facebook의 문장 임베딩 방식은 단순히 단어 수준에서의 유사성뿐만 아니라 전체 문장의 맥락과 의미를 반영할 수 있다는 점에서 차별화된다. 이는 문장 전체의 의미를 보존하면서도 다국어 간 유사성을 효과적으로 평가할 수 있게 한다.
